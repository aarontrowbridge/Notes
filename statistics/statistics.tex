\documentclass{article}

\usepackage[bookmarks]{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}

\newtheorem{mydef}{Definition}

\newcommand{\xbar}{\bar x}


% \usepackage{pst-poker}
% \psset{linewidth=25pt}

\linespread{1.2}


\title{Notes: Statistics}
\author{Aaron Trowbridge}
\date{}

\setcounter{section}{-1}

\begin{document}
\maketitle

\pagenumbering{roman}
\tableofcontents
\newpage


\pagenumbering{arabic}


\section{Introduction}

In the beginning there was a population, and over that population, there was a distribution.  As mortals, and not gods, if we want to try to understand this distribution the best we can do is attempt to observe features of the distribution--hopefully at random--and use that data to build a blurry picture of the truth lying behind our perception.  

Dramatization aside, this is the principle assumption and goal of statistics: infer knowledge about a hidden (parameterized) distribution by taking random samples and applying mathematical techniques to the data obtained. Usually we are either trying to estimate the \textit{true} hidden parameters or test hypotheses we might have about those parameters. 

In these notes I intend to condense the essential methods needed to piece together an actionable representation of the uncertain world around us.  Statistical analysis of data is an essential tool in almost every field imaginable: from physics to psychology to finance, and everywhere in between.

Throughout these notes I will include custom visualizations and code snippets, for which I will be using the Julia programming language.  Code for everything in this document, including the \LaTeX \  source code, can be found on my github. 

\newpage
\part{Data: Production, Visualization, and Description}
\section{Producing Data}

There are many ways to get data, not all are equal, and, even more importantly, not all are equally as useful. \textit{Bias} can seep into data in a number of ways depending on how the data is obtained. 

The first step in getting data is to decide on the \textit{population} we are interested in, which is typically very large (if it weren't we could just sample the whole population and there would be no need for statistics!). 

The next step is to decide what kind of data we are interested in: namely either \textit{numerical} (e.g. height or weight) or \textit{categorical} (e.g. political party or favorite flavor of ice cream). Denoting our \textit{random} variable of interest as $X$: 

\begin{mydef}[Numerical Variable] 
A feature obtained from an observation which has a random continuous numerical value according to the underlying distribution, e.g. $X \sim N(\mu, \sigma)$.
\end{mydef}

\begin{mydef}[Categorical Variable] 
A feature obtained from an observation which takes it's value from a set of possible categories $\{x_1, x_2, \dots , x_k\}$ and $P(X = x_i) = p_i$
\end{mydef}

The final step is to decide how we plan on getting our data; procedures for which can be broken up into two categories: \textit{sampling} and \textit{experimenting} 

\subsection{Sampling}

Since populations are so large, and we can't just issue a census, we need a way to get a representative \textit{sample} (say of size $n$) of the population, that we can analyze.  There are various ways to sample and each comes with it's advantages and disadvantages. Here we list some of the ``good'' sampling methods:

\begin{description}[style=nextline]
\item [Simple Random Sample (SRS)] This is the ideal sample and involves enumerating all of the possible members of the population and randomly selecting $n$ numbers in the range of the population size.

\item [Survey Sample] Here $n$ individuals are chosen using a SRS (ideally) and asked to complete a survey, which can result in numerical or categorical data or both. This is a ``good'' sample if all who receive a survey complete it.

\item [Stratified Random Sample] In this set up the population is segmented into some number of \textit{strata} and then a SRS used to gather data from each.
    
\item [Multistage Sample] Iterated grouping and random sampling with multiple layers.

\end{description}

\noindent Bias, where certain outcomes are systematically favored, can accrue when the sampling method is not random.  Here I will list some of the types of bias that can arise:

\begin{description}[style=nextline]
\item [Convenience Bias] Individuals are chosen based on ease of access.

\item [Response Bias] Individuals are issued a survey and have an option not to complete the survey.

\item [Undercoverage Bias] When some individuals in a population are not accessible due to the design of a survey.

    
\end{description}

\noindent 
\begin{description}[style=nextline]
\item [] 

\end{description}


\subsection{Experimenting}


\section{Data Visualization}
\section{Descriptive Statistics}

\begin{mydef}[Unbiased Estimator]
A statistic (e.g. $\xbar$) used to estimate a population parameter (e.g. $\mu$) s.t. $E(\xbar) = \mu$ 
\end{mydef}

\newpage
\part{Probability Theory}
\section{Probability}
\section{Random Variables}
\section{Expectation}
\section{Important Inequalities}
\section{Convergence Results}
\section{Distributions}
\subsection{Normal Distribution}
\subsection{$t$-Distribution}
\subsection{$\chi^2$-Distribution}

\newpage
\part{Statistical Inference}

Thus far we have looked at how to extract data from a population, describe the data with summary statistics, and visualize the data with various types of plots.  We will assume the obtained data had to come from a distribution that we don't have direct access to -- we can only sample from it.  Another assumption we will make is that the hidden distribution can be \textit{parameterized}, that is there is a finite set of numbers that perfectly describe the distribution that we just don't know (a seemingly flimsy assumption but one that is necessary and fairly powerful). Now we will turn or attention to the principle task of statistics: inferring knowledge about the underlying distribution's parameters from a sample.

\section{Sampling Distribution}

Let's assume that the distribution we are after can be described by it's position, or mean $\mu$, and it's standard deviation $\sigma$.  Which means our random variable $X \sim Distribution(\mu, \sigma)$ will give us values of $x_i$ for the $i$-th measurement.  If we take a sample of size $n$ and calculate the \textit{sample mean}

\begin{equation}
\xbar = \frac{1}{n} \sum_{i=1}^n x_i.
\end{equation}

\noindent
The central limit theorem tells us that as $n$ gets large, 

\begin{equation}
\label{eq:sample_dist}
\xbar \sim N(\mu, \sigma / \sqrt{n}), 
\end{equation}


\noindent
which implies that

\begin{equation}
z = \frac{\xbar - \mu}{\sigma \sqrt{n}} \sim N(0,1)
\end{equation}

\noindent
has a standard normal distribution which we can use to easily do calculations.



\newpage
\section{Parameter Estimation: Confidence Intervals}

If our goal is to estimate the value of some parameter, the population mean $\mu$ for instance, we can take a sample and calculate the sample mean $\xbar$, and use the fact stemming from it's distribution (\ref{eq:sample_dist}), that $\mathbb{E}(\xbar) = \mu$, to construct a range around $\xbar$ s.t. we can be confidant that $\mu$ lies within that range. In general a confidence interval will look like

\begin{equation}
    \text{estimate} \pm \text{margin of error}
\end{equation}

\subsubsection*{One sample $z$-confidence interval}

Given a sample $\{x_1, x_2, \dots, x_n\}$ taken from a population with mean $\mu$ and known standard deviation $\sigma$, we can calculate a $\gamma$-level confidence interval around $\xbar$,  given by

\begin{equation}
\boxed{
\xbar \pm z^* \frac{\sigma}{\sqrt{n}}
}
\end{equation}

\noindent
Here, $z^*$ is the critical value defined implicitly by 

\begin{equation}
\gamma = \text{area under standard normal curve between } -z^* \text{ and } z^* 
\end{equation}

\noindent
For example, if $\gamma = .6 = 60\%$, then $z* = 1$, by the 60-95-99.7 rule. 


\subsubsection*{One sample $t$-confidence interval}

If we don't know $\sigma$ then all is not lost, we simply use the sample standard deviation

\begin{equation}
s = \sqrt{ \frac{1}{n - 1} \sum_{i=1}^n (x_i - \xbar)^2}
\end{equation}

\noindent
in place of $\sigma$ and 

\begin{equation}
t = \frac{\xbar - \mu}{s \sqrt{n}} 
\end{equation}

\noindent
in place of $z$ which has a $t$-distribution, with $\nu = n - 1$ degrees of freedom.  The $\gamma$-level confidence interval in this case is then given by

\begin{equation}
\boxed{
\xbar \pm t^* \frac{s}{\sqrt{n}} 
}
\end{equation}

\noindent
where $t^*$ is defined analogously to $z^*$. 


\subsubsection*{Two sample $t$-confidence interval}

Given two samples of size $n_1$ and $n_2$, respectively drawn from two populations with means $\mu_1$ and $\mu_2$, we can find a $\gamma$-level confidence interval for $\mu_1 - \mu_2$, given by 

\begin{equation}
\boxed{
(\xbar_1 - \xbar_2) \pm t^* \sqrt{\frac{s_1^2}{n_1} +\frac{s_2^2}{n_2}} 
}
\end{equation}

\noindent
where $t^*$ is found using the $t$-distribution with $\nu = \min\{n_1 - 1, n_2 - 1\}$

\section{Hypothesis Testing}

\newpage
\part{Statistical Models and Methods}
\section{Regression}
\section{Classification}

    
\end{document}